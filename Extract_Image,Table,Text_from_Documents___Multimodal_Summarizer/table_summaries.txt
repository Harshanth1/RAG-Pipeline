Table summarizing **translation performance (BLEU/COMET scores)** of LLMs (XGLM-7.5B, OPT-175B, Falcon-7B, LLaMA2-7B, LLaMA2-7B-Chat, ChatGPT, GPT-4) and MT models (M2M-12B, NLLB-1.3B, Google) across **language families** (Indo-Euro subfamilies, Austronesian, Atlantic-Congo, Afro-Asiatic, Turkic, Dravidian, Sino-Tibetan, Other) for **X⇒Eng** and **Eng⇒X** directions, with language counts in parentheses. Top performers: **GPT-4** and **Google** lead in most high-resource Indo-European families; **NLLB/Google** strong in low-resource (e.g., Dravidian, Atlantic-Congo); consistent **X⇒Eng** superiority over **Eng⇒X**.
Table comparing **SEScore translation performance** (X⇒Eng, lower negative better) of LLMs (XGLM-7.5B, OPT-175B, Falcon-7B, LLaMA-7B variants, ChatGPT, GPT4) and MT models (M2M-12B, NLLB-1.3B, Google) across **13 language families** (e.g., Indo-European Germanic/Romance/Slavic/Indo-Aryan, Austronesian, Atlantic-Congo), with sample sizes in parentheses. **GPT4/Google best overall**; Romance/Germanic easiest, low-resource (Dravidian/Afro-Asiatic) hardest.
Table of **language family translation accuracies** (%) between **English (Eng)**, **French (Fra)**, **Chinese (Zho)**, and language X (with sample sizes in parentheses). Columns: X⇒Eng, X⇒Fra, X⇒Zho, Eng⇒X, Fra⇒X, Zho⇒X. **Highest performers**: Indo-Euro-Germanic (~48% X⇒Eng, 45% X⇒Fra), Indo-Euro-Romance; **lowest**: Atlantic-Congo (~28% X⇒Eng, 8% Zho⇒X). Families include Indo-Euro subgroups, Austronesian, Atlantic-Congo, Afro-Asiatic, Turkic, Dravidian, Sino-Tibetan, Other.
Table comparing **COMET scores** for **in-context translation templates** across **Deu-Eng, Eng-Deu, Rus-Eng, Eng-Rus, Rus-Deu, Deu-Rus** language pairs and **average**. **Reasonable instructions** (e.g., "<X>=<Y>", "Translate from [SRC] to [TGT]") achieve higher averages (~24) than **unreasonable instructions** (~20-25), with top performers like "<X>=<Y>" (25.12 reasonable) and lowest like "[SRC]: <X> \n [SRC]: <Y>" (10.26 unreasonable).[1]
Table evaluating performance of in-context exemplars for text summarization across **consistency, granularity, diversity** under various translation mismatch types (Mismatched, Word-level, Doc-level, Duplicated, Sent-level Translation) on language pairs **Deu-Eng, Eng-Deu, Zho-Eng, Eng-Zho**. Exemplar configs: xK (low consistency?), v (high?). Scores (likely error rates): range 0.00 (Mismatched Deu-Eng) to 37.37 (Sent-level Deu-Eng); **Duplicated Translation** worst overall (e.g., 35.12 Deu-Eng), **Mismatched** best low-resource pairs.
Table of **Rev Deu-Eng and Eng-Deu ratios** (Head/Tail metrics) across 8 progressive fractions (0/8 to 8/8), showing sharp declines: Deu-Eng Head from 37.37 to 3.42, Tail from 37.37 to 3.42; Eng-Deu Head from 26.49 to 3.10, Tail from 26.49 to 3.10.
Table of average **X⇒Eng (BLEU)** and **Eng⇒X (BLEU)** translation scores for language families (e.g., Indo-European-Germanic (8 langs), Romance (8), Slavic (12), Austronesian (6), Afro-Asiatic (6)) across models including **XGLM-7.5B**, **OPT-175B**, **Falcon-7B**, **LLaMA2-7B**, **ChatGPT**, **GPT4**, **M2M-12B**, **NLLB-1.3B**, **Google**. Individual language scores (e.g., afr, dan, hin, zho) precede family averages; top performers: GPT4/Google (high 40-60s BLEU), low for Sino-Tibetan/Other (~1-30).
Table of average performance scores (likely accuracy percentages) across language families and specific languages, grouped by Indo-European sub-branches (Germanic: 7 langs avg ~84-89%; Romance: 4 avg ~88%; Slavic: 12 avg ~83-88%; Other IE: 9 avg ~80-88%), plus Austronesian (3 avg ~66%), Atlantic-Congo (2 avg ~60%), Afro-Asiatic (4 avg ~73%), Turkic (5 avg ~82%), Dravidian (4 avg ~56%), Sino-Tibetan (3 avg ~88%), and Other (13 avg ~75%). Individual scores range 0.28-92.42 for languages like nld/deu (Dutch/German high 80s-90s), hin (Hindi ~70s), zho (Chinese ~89%), jpn/kor (Japanese/Korean ~80s), with Germanic/Romance/Slavic consistently highest (~85-90 avg), non-IE lower variance. Optimized tags: language family benchmarks, Indo-European Germanic Romance Slavic averages, multilingual model evaluation scores.
This table presents **machine translation evaluation scores** across multiple language families and models, measuring translation quality (likely via negative error metrics where values closer to zero indicate better performance).

## Key Structure

The table organizes languages by linguistic family and evaluates nine translation systems:
- Large language models: XGLM-7.5B, OPT-175B, Falcon-7B, LLaMA2-7B variants, ChatGPT, GPT-4
- Specialized translation models: M2M-12B, NLLB-1.3B, Google Translate

## Language Families Covered

- **Indo-European subfamilies**: Germanic, Romance, Slavic, Indo-Aryan, and Other (11 languages total)
- **Non-Indo-European families**: Austronesian, Atlantic-Congo, Afro-Asiatic, Turkic, Dravidian, Sino-Tibetan
- **Isolated/Other languages**: Japanese, Korean, Vietnamese, Thai, and others

## Performance Patterns

Scores vary significantly by language family and model. Generally, modern models (ChatGPT, GPT-4, NLLB-1.3B) show better performance (scores closer to zero) compared to earlier models. Indo-European languages, particularly Germanic and Romance branches, tend to have higher scores (better performance) than more distant language families. Specialized translation models often outperform general-purpose LLMs on this task.
**Table of ~120 world languages with ISO 639-1 (2-letter), ISO 639-2/T (3-letter terminology) codes, and language families.** Includes Indo-European (Germanic, Romance, Slavic, Indo-Aryan, Other), Afro-Asiatic, Atlantic-Congo, Austronesian, Dravidian, Sino-Tibetan, Turkic, Other; examples: Afrikaans (af/afr, Indo-European-Germanic), Arabic (ar/ara, Afro-Asiatic), Chinese variants (zh/zho_simpl, zhtrad/zho_trad, Sino-Tibetan), Swahili (sw/swh, Atlantic-Congo).[1][7]
